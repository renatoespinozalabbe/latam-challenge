{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREGUNTA 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días. Debe incluir las siguientes funciones:\n",
    "```python\n",
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "```\n",
    "```python\n",
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "```\n",
    "```python\n",
    "Returns: \n",
    "[(datetime.date(1999, 11, 15), \"LATAM321\"), (datetime.date(1999, 7, 15), \"LATAM_CHI\"), ...]\n",
    "```\n",
    "## PREGUNTA 1 - TIME (q1_time)\n",
    "Para desarrollar la solución a este problema se utilizará una solución cloud basada en Google Cloud Platform. La función contendrá el proceso de ETL para llevar los datos desde el archivo json local hacia Google Cloud Storage y posteriormente modelar esa data en una tabla de bigquery que permita realizar consultas de manera rápida y eficiente.\n",
    "### ETL\n",
    "El proceso de extracción, transformación y carga de los archivos en GCP permite llevar los archivos a una plataforma de performance rápida y efectiva.\n",
    "#### Credenciales\n",
    "Este desarrollo se realizará utilizando Google Cloud, por lo que se crea un proyecto en GCP llamado \"project-latam-challenge\". En este proyecto se crea una service account llamada \"sa-etl-latam-challenge\" que será utilizada para realizar la carga de datos.\n",
    "```python\n",
    "import os\n",
    "\n",
    "# Ruta a archivo de credenciales JSON de Google Cloud\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"../credentials/project-latam-challenge-749ce1a96052.json\"\n",
    "```\n",
    "#### Carga en Dataframe\n",
    "Se carga en un dataframe el [archivo](https://drive.google.com/file/d/1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis/view?usp=sharing) json declarado como parte del challenge.\n",
    "```python\n",
    "    # Leer el archivo CSV en un DataFrame\n",
    "    df = pd.read_json(file_path,lines=True)\n",
    "```\n",
    "#### Carga de dataframe en GCP\n",
    "##### Carga de archivo en Google Cloud Storage (GCS)\n",
    "Usando Google Cloud SDK se crea un bucket llamado 'bucket-project-latam-challenge-q1-time' en el proyecto 'project-latam-challenge' y se carga el archivo directamente a dicho bucket.\n",
    "```python\n",
    "    # Se define función que permite cargar un archivo en un bucket de Google Cloud Storage\n",
    "    def create_bucket_and_upload_file(project_id, bucket_name, file_path, destination_blob_name):\n",
    "        # Inicializar el cliente de almacenamiento\n",
    "        storage_client = storage.Client(project=project_id)\n",
    "        \n",
    "        # Verificar si el bucket ya existe\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        if not bucket.exists():\n",
    "            # Crear un nuevo bucket con la ubicación especificada\n",
    "            new_bucket = storage_client.create_bucket(bucket, location=\"US\")\n",
    "            print(f'Bucket {bucket_name} created in location US.')\n",
    "        else:\n",
    "            print(f'Bucket {bucket_name} already exists.')\n",
    "            new_bucket = bucket\n",
    "        \n",
    "        # Subir el archivo al bucket\n",
    "        blob = new_bucket.blob(destination_blob_name)\n",
    "        blob.upload_from_filename(file_path)\n",
    "        print(f'File {file_path} uploaded to {bucket_name}/{destination_blob_name}.')\n",
    "```\n",
    "##### Configuración de carga\n",
    "```python\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Parámetros\n",
    "project_id = 'project-latam-challenge'\n",
    "dataset_id = 'twitter_data'\n",
    "table_id = 'farmers_protest_tweets_2021'\n",
    "```\n",
    "##### Declaración de schema\n",
    "Se declara explicitamente la estructura del esquema con la data correspondiente, esto permitirá evitar errores en los tipos de dato al cargar la data en Bigquery.\n",
    "Por motivos de claridad al momento de leer el markdown, se omite el schema que se puede encontrar en el archivo q1_time.py .\n",
    "##### Proceso de creación de tabla en bigquery\n",
    "```python\n",
    "    # Se define función que crea tabla de bigquery a partir de archivo almacenado en GCS\n",
    "    def load_data_from_gcs_to_bigquery(uri, table_id):\n",
    "        # Inicializa el cliente de BigQuery\n",
    "        client = bigquery.Client()\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            schema=schema,\n",
    "            source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "            max_bad_records=0,  # No permitir registros malos antes de fallar\n",
    "            time_partitioning=bigquery.TimePartitioning(\n",
    "                type_=bigquery.TimePartitioningType.DAY,\n",
    "                field=\"date\"  # Campo de partición\n",
    "            )\n",
    "        )\n",
    "\n",
    "        load_job = client.load_table_from_uri(\n",
    "            uri, table_id, job_config=job_config\n",
    "        )\n",
    "\n",
    "        print(f'Starting job {load_job.job_id}')\n",
    "        load_job.result()\n",
    "        print(f'Job finished.')\n",
    "\n",
    "        destination_table = client.get_table(table_id)\n",
    "        print(f'Loaded {destination_table.num_rows} rows.')\n",
    "```\n",
    "##### Análisis\n",
    "```python\n",
    "from google.cloud import bigquery\n",
    "from typing import List, Tuple\n",
    "import datetime\n",
    "\n",
    "def get_bigquery_client():\n",
    "    return bigquery.Client()\n",
    "\n",
    "def run_bigquery_query(query: str):\n",
    "    client = get_bigquery_client()\n",
    "    query_job = client.query(query)\n",
    "    results = query_job.result()\n",
    "    return results\n",
    "\n",
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    query = \"\"\"\n",
    "    WITH TEMP_DATA_001 AS \n",
    "    (\n",
    "      SELECT DATE(date) AS fecha,\n",
    "        id,\n",
    "        user.username AS username\n",
    "      FROM `project-latam-challenge.twitter_data.farmers_protest_tweets_2021` \n",
    "      WHERE DATE(date) IS NOT NULL\n",
    "    ), TEMP_DATE_001 AS \n",
    "    (\n",
    "      SELECT fecha,\n",
    "        COUNT(DISTINCT id) AS tweet_qty\n",
    "      FROM TEMP_DATA_001\n",
    "      GROUP BY fecha\n",
    "    ), TEMP_DATE_002 AS \n",
    "    (\n",
    "      SELECT A.*,\n",
    "        RANK() OVER(ORDER BY tweet_qty DESC, RAND()) AS ranking_tweet\n",
    "      FROM TEMP_DATE_001 A\n",
    "    ), TEMP_USER_001 AS \n",
    "    (\n",
    "      SELECT fecha,\n",
    "        username,\n",
    "        COUNT(DISTINCT id) AS tweet_qty\n",
    "      FROM TEMP_DATA_001\n",
    "      GROUP BY fecha, username\n",
    "    ), TEMP_USER_002 AS \n",
    "    (\n",
    "      SELECT A.*,\n",
    "        RANK() OVER(PARTITION BY fecha ORDER BY tweet_qty DESC, RAND()) AS ranking_user\n",
    "      FROM TEMP_USER_001 A\n",
    "    ), TEMP_USER_003 AS \n",
    "    (\n",
    "      SELECT A.*\n",
    "      FROM TEMP_USER_002 A\n",
    "      WHERE ranking_user=1\n",
    "    )\n",
    "    SELECT A.fecha,\n",
    "      B.username\n",
    "    FROM TEMP_DATE_002 A\n",
    "    LEFT JOIN TEMP_USER_003 B\n",
    "    ON A.fecha=B.fecha \n",
    "    WHERE A.ranking_tweet<=10\n",
    "    ORDER BY ranking_tweet\n",
    "    \"\"\"\n",
    "    \n",
    "    results = run_bigquery_query(query)\n",
    "    return [(row.fecha, row.username) for row in results]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '../data/farmers_protest_tweets_2021.json'\n",
    "    top_10_dates = q1_time(file_path)\n",
    "    for date, user in top_10_dates:\n",
    "        print(f\"Fecha: {date}, Usuario: {user}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso q1_time...\n",
      "Getting BigQuery client\n",
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "Proceso completado.\n",
      "Fecha: 2021-02-12, Usuario: RanbirS00614606\n",
      "Fecha: 2021-02-13, Usuario: MaanDee08215437\n",
      "Fecha: 2021-02-17, Usuario: RaaJVinderkaur\n",
      "Fecha: 2021-02-16, Usuario: jot__b\n",
      "Fecha: 2021-02-14, Usuario: rebelpacifist\n",
      "Fecha: 2021-02-18, Usuario: neetuanjle_nitu\n",
      "Fecha: 2021-02-15, Usuario: jot__b\n",
      "Fecha: 2021-02-20, Usuario: MangalJ23056160\n",
      "Fecha: 2021-02-23, Usuario: Surrypuria\n",
      "Fecha: 2021-02-19, Usuario: Preetm91\n"
     ]
    }
   ],
   "source": [
    "# Importa los módulos necesarios\n",
    "import os\n",
    "from q1_time import q1_time\n",
    "\n",
    "# Establece la ruta del archivo de credenciales\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"d:/valen/Documents/GitHub/latam-challenge/credentials/project-latam-challenge-749ce1a96052.json\"\n",
    "\n",
    "# Ruta al archivo JSON\n",
    "file_path = 'd:/valen/Documents/GitHub/latam-challenge/src/farmers-protest-tweets-2021-2-4.json'\n",
    "\n",
    "# Llama a la función q1_time\n",
    "print(\"Iniciando proceso q1_time...\")\n",
    "top_10_dates = q1_time(file_path)\n",
    "print(top_10_dates)\n",
    "print(\"Proceso completado.\")\n",
    "\n",
    "# Imprime los resultados\n",
    "for date, user in top_10_dates:\n",
    "    print(f\"Fecha: {date}, Usuario: {user}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
