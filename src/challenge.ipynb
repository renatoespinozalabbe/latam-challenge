{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREGUNTA 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días. Debe incluir las siguientes funciones:\n",
    "```python\n",
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "```\n",
    "```python\n",
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "```\n",
    "```python\n",
    "Returns: \n",
    "[(datetime.date(1999, 11, 15), \"LATAM321\"), (datetime.date(1999, 7, 15), \"LATAM_CHI\"), ...]\n",
    "```\n",
    "## PREGUNTA 1 - TIME (q1_time)\n",
    "Para desarrollar la solución a este problema se utilizará una solución cloud basada en Google Cloud Platform. La función contendrá el proceso de ETL para llevar los datos desde el archivo json local hacia Google Cloud Storage y posteriormente modelar esa data en una tabla de bigquery que permita realizar consultas de manera rápida y eficiente.\n",
    "### ETL\n",
    "El proceso de extracción, transformación y carga de los archivos en GCP permite llevar los archivos a una plataforma de performance rápida y efectiva.\n",
    "#### Credenciales\n",
    "Este desarrollo se realizará utilizando Google Cloud, por lo que se crea un proyecto en GCP llamado \"project-latam-challenge\". En este proyecto se crea una service account llamada \"sa-etl-latam-challenge\" que será utilizada para realizar la carga de datos.\n",
    "```python\n",
    "import os\n",
    "\n",
    "# Ruta a archivo de credenciales JSON de Google Cloud\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"../credentials/project-latam-challenge-749ce1a96052.json\"\n",
    "```\n",
    "#### Carga en Dataframe\n",
    "Se carga en un dataframe el [archivo](https://drive.google.com/file/d/1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis/view?usp=sharing) json declarado como parte del challenge.\n",
    "```python\n",
    "    # Leer el archivo CSV en un DataFrame\n",
    "    df = pd.read_json(file_path,lines=True)\n",
    "```\n",
    "#### Carga de dataframe en GCP\n",
    "##### Carga de archivo en Google Cloud Storage (GCS)\n",
    "Usando Google Cloud SDK se crea un bucket llamado 'bucket-project-latam-challenge-q1-time' en el proyecto 'project-latam-challenge' y se carga el archivo directamente a dicho bucket.\n",
    "```python\n",
    "    # Se define función que permite cargar un archivo en un bucket de Google Cloud Storage\n",
    "    def create_bucket_and_upload_file(project_id, bucket_name, file_path, destination_blob_name):\n",
    "        # Inicializar el cliente de almacenamiento\n",
    "        storage_client = storage.Client(project=project_id)\n",
    "        \n",
    "        # Verificar si el bucket ya existe\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        if not bucket.exists():\n",
    "            # Crear un nuevo bucket con la ubicación especificada\n",
    "            new_bucket = storage_client.create_bucket(bucket, location=\"US\")\n",
    "            print(f'Bucket {bucket_name} created in location US.')\n",
    "        else:\n",
    "            print(f'Bucket {bucket_name} already exists.')\n",
    "            new_bucket = bucket\n",
    "        \n",
    "        # Subir el archivo al bucket\n",
    "        blob = new_bucket.blob(destination_blob_name)\n",
    "        blob.upload_from_filename(file_path)\n",
    "        print(f'File {file_path} uploaded to {bucket_name}/{destination_blob_name}.')\n",
    "```\n",
    "##### Configuración de carga\n",
    "```python\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Parámetros\n",
    "project_id = 'project-latam-challenge'\n",
    "dataset_id = 'twitter_data'\n",
    "table_id = 'farmers_protest_tweets_2021'\n",
    "```\n",
    "##### Declaración de schema\n",
    "Se declara explicitamente la estructura del esquema con la data correspondiente, esto permitirá evitar errores en los tipos de dato al cargar la data en Bigquery.\n",
    "Por motivos de claridad al momento de leer el markdown, se omite el schema que se puede encontrar en el archivo q1_time.py .\n",
    "##### Proceso de creación de tabla en bigquery\n",
    "```python\n",
    "    # Se define función que crea tabla de bigquery a partir de archivo almacenado en GCS\n",
    "    def load_data_from_gcs_to_bigquery(uri, table_id):\n",
    "        # Inicializa el cliente de BigQuery\n",
    "        client = bigquery.Client()\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            schema=schema,\n",
    "            source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "            max_bad_records=0,  # No permitir registros malos antes de fallar\n",
    "            time_partitioning=bigquery.TimePartitioning(\n",
    "                type_=bigquery.TimePartitioningType.DAY,\n",
    "                field=\"date\"  # Campo de partición\n",
    "            )\n",
    "        )\n",
    "\n",
    "        load_job = client.load_table_from_uri(\n",
    "            uri, table_id, job_config=job_config\n",
    "        )\n",
    "\n",
    "        print(f'Starting job {load_job.job_id}')\n",
    "        load_job.result()\n",
    "        print(f'Job finished.')\n",
    "\n",
    "        destination_table = client.get_table(table_id)\n",
    "        print(f'Loaded {destination_table.num_rows} rows.')\n",
    "```\n",
    "##### Análisis\n",
    "Se define función que permite ejecutar la consulta en bigquery.\n",
    "\n",
    "```python\n",
    "    # Se define función para ejecutar una consulta en BigQuery\n",
    "    def run_bigquery_query(query: str):\n",
    "        client = get_bigquery_client()\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "        return results\n",
    "```\n",
    "Se define la query SQL que permite ejecutar la consulta en bigquery.\n",
    "```python\n",
    "    query = \"\"\"\n",
    "        WITH TEMP_DATA_001 AS \n",
    "        (\n",
    "        -- Se crea la tabla temporal sólo con los campos necesarios para realizar ambos cálculos, tanto por fecha como por usuario\n",
    "        SELECT DATE(date) AS fecha,\n",
    "            id,\n",
    "            user.username AS username\n",
    "        FROM `project-latam-challenge.twitter_data.farmers_protest_tweets_2021` \n",
    "        WHERE DATE(date) IS NOT NULL\n",
    "        ), TEMP_DATE_001 AS \n",
    "        (\n",
    "        -- Se realiza el cálculo de la cantidad de tweets por fecha\n",
    "        SELECT fecha,\n",
    "            COUNT(DISTINCT id) AS tweet_qty\n",
    "        FROM TEMP_DATA_001\n",
    "        GROUP BY fecha\n",
    "        ), TEMP_DATE_002 AS \n",
    "        (\n",
    "        -- Se realiza el ranking de tweets ordenados descendentemente por cantidad de tweets y aleatoriamente\n",
    "        SELECT A.*,\n",
    "            RANK() OVER(ORDER BY tweet_qty DESC, RAND()) AS ranking_tweet \n",
    "        FROM TEMP_DATE_001 A\n",
    "        ), TEMP_USER_001 AS \n",
    "        (\n",
    "        -- Se realiza el cálculo de la cantidad de tweets por usuario y fecha\n",
    "        SELECT fecha,\n",
    "            username,\n",
    "            COUNT(DISTINCT id) AS tweet_qty\n",
    "        FROM TEMP_DATA_001\n",
    "        GROUP BY fecha, username\n",
    "        ), TEMP_USER_002 AS \n",
    "        -- Se realiza el ranking de usuarios por fecha, ordenados descendentemente por cantidad de tweets y aleatoriamente\n",
    "        (\n",
    "        SELECT A.*,\n",
    "            RANK() OVER(PARTITION BY fecha ORDER BY tweet_qty DESC, RAND()) AS ranking_user\n",
    "        FROM TEMP_USER_001 A\n",
    "        ), TEMP_USER_003 AS \n",
    "        (\n",
    "        -- Se seleccionan los usuarios con ranking 1, es decir, aquellos que más tweets hicieron por día\n",
    "        SELECT A.*\n",
    "        FROM TEMP_USER_002 A\n",
    "        WHERE ranking_user=1\n",
    "        )\n",
    "        -- Se filtra la tabla temporal de tweets por fecha, seleccionando sólo los días que se encuentran en el top 10\n",
    "        SELECT A.fecha,\n",
    "            B.username\n",
    "        FROM TEMP_DATE_002 A\n",
    "        LEFT JOIN TEMP_USER_003 B\n",
    "        ON A.fecha=B.fecha \n",
    "        WHERE A.ranking_tweet<=10\n",
    "        ORDER BY ranking_tweet\n",
    "        \"\"\"\n",
    "    \n",
    "    results = run_bigquery_query(query)\n",
    "    return [(row.fecha, row.username) for row in results]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso q1_time...\n",
      "Leyendo archivo JSON\n",
      "Creando bucket y subiendo archivo\n",
      "Bucket bucket-project-latam-challenge-q1-time created in location US.\n",
      "File farmers-protest-tweets-2021-2-4.json uploaded to bucket-project-latam-challenge-q1-time/farmers-protest-tweets-2021-2-4.json.\n",
      "Cargando datos a BigQuery\n",
      "Starting job 22e9f6c9-c732-4439-b44f-a688290fc0ea\n",
      "Job finished.\n",
      "Loaded 117407 rows.\n",
      "Ejecutando query\n",
      "Resultados:\n",
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "Tiempo de ejecución: 59.299320936203 segundos\n",
      "Uso máximo de memoria: 2244.50390625 MiB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from q1_time import q1_time\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"../credentials/project-latam-challenge-749ce1a96052.json\"\n",
    "\n",
    "# Función para medir el tiempo y la memoria de q1_time\n",
    "def measure_q1_time(file_path):\n",
    "    start_time = time.time()\n",
    "    mem_usage, result = memory_usage((q1_time, (file_path,)), retval=True, max_usage=True)\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    return result, duration, mem_usage\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Iniciando proceso q1_time...\")\n",
    "    top_10_dates, duration, mem_usage = measure_q1_time(file_path)\n",
    "    print(\"Resultados:\")\n",
    "    print(top_10_dates)\n",
    "    print(f\"Tiempo de ejecución: {duration} segundos\")\n",
    "    print(f\"Uso máximo de memoria: {mem_usage} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREGUNTA 1 - MEMORY\n",
    "Se propone generar un proceso análogo al anteriormente realizado para la versión time, pero con una carga parcelada de forma que utilice menos memoria local y se ejecute con menor necesidad de recursos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Los top 10 emojis más usados con su respectivo conteo. Debe incluir las siguientes funciones:\n",
    "```python\n",
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "```\n",
    "```python\n",
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "```\n",
    "```python\n",
    "Returns: \n",
    "[(\"✈️\", 6856), (\"❤️\", 5876), ...]\n",
    "```\n",
    "## PREGUNTA 2 - TIME (q2_time)\n",
    "Para desarrollar la solución a este problema se utilizará una solución cloud basada en Google Cloud Platform. La función contendrá el proceso de ETL para llevar los datos desde el archivo json local hacia Google Cloud Storage y posteriormente modelar esa data en una tabla de bigquery que permita realizar consultas de manera rápida y eficiente.\n",
    "### ETL\n",
    "El proceso de extracción, transformación y carga de los archivos en GCP permite llevar los archivos a una plataforma de performance rápida y efectiva.\n",
    "#### Credenciales\n",
    "Este desarrollo se realizará utilizando Google Cloud, por lo que se crea un proyecto en GCP llamado \"project-latam-challenge\". En este proyecto se crea una service account llamada \"sa-etl-latam-challenge\" que será utilizada para realizar la carga de datos.\n",
    "```python\n",
    "import os\n",
    "\n",
    "# Ruta a archivo de credenciales JSON de Google Cloud\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"../credentials/project-latam-challenge-749ce1a96052.json\"\n",
    "```\n",
    "#### Carga en Dataframe\n",
    "Se carga en un dataframe el [archivo](https://drive.google.com/file/d/1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis/view?usp=sharing) json declarado como parte del challenge.\n",
    "```python\n",
    "    # Leer el archivo CSV en un DataFrame\n",
    "    df = pd.read_json(file_path,lines=True)\n",
    "```\n",
    "#### Carga de dataframe en GCP\n",
    "##### Carga de archivo en Google Cloud Storage (GCS)\n",
    "Usando Google Cloud SDK se crea un bucket llamado 'bucket-project-latam-challenge-q1-time' en el proyecto 'project-latam-challenge' y se carga el archivo directamente a dicho bucket.\n",
    "```python\n",
    "    # Se define función que permite cargar un archivo en un bucket de Google Cloud Storage\n",
    "    def create_bucket_and_upload_file(project_id, bucket_name, file_path, destination_blob_name):\n",
    "        # Inicializar el cliente de almacenamiento\n",
    "        storage_client = storage.Client(project=project_id)\n",
    "        \n",
    "        # Verificar si el bucket ya existe\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        if not bucket.exists():\n",
    "            # Crear un nuevo bucket con la ubicación especificada\n",
    "            new_bucket = storage_client.create_bucket(bucket, location=\"US\")\n",
    "            print(f'Bucket {bucket_name} created in location US.')\n",
    "        else:\n",
    "            print(f'Bucket {bucket_name} already exists.')\n",
    "            new_bucket = bucket\n",
    "        \n",
    "        # Subir el archivo al bucket\n",
    "        blob = new_bucket.blob(destination_blob_name)\n",
    "        blob.upload_from_filename(file_path)\n",
    "        print(f'File {file_path} uploaded to {bucket_name}/{destination_blob_name}.')\n",
    "```\n",
    "##### Configuración de carga\n",
    "```python\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Parámetros\n",
    "project_id = 'project-latam-challenge'\n",
    "dataset_id = 'twitter_data'\n",
    "table_id = 'farmers_protest_tweets_2021'\n",
    "```\n",
    "##### Declaración de schema\n",
    "Se declara explicitamente la estructura del esquema con la data correspondiente, esto permitirá evitar errores en los tipos de dato al cargar la data en Bigquery.\n",
    "Por motivos de claridad al momento de leer el markdown, se omite el schema que se puede encontrar en el archivo q1_time.py .\n",
    "##### Proceso de creación de tabla en bigquery\n",
    "```python\n",
    "    # Se define función que crea tabla de bigquery a partir de archivo almacenado en GCS\n",
    "    def load_data_from_gcs_to_bigquery(uri, table_id):\n",
    "        # Inicializa el cliente de BigQuery\n",
    "        client = bigquery.Client()\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            schema=schema,\n",
    "            source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "            max_bad_records=0,  # No permitir registros malos antes de fallar\n",
    "            time_partitioning=bigquery.TimePartitioning(\n",
    "                type_=bigquery.TimePartitioningType.DAY,\n",
    "                field=\"date\"  # Campo de partición\n",
    "            )\n",
    "        )\n",
    "\n",
    "        load_job = client.load_table_from_uri(\n",
    "            uri, table_id, job_config=job_config\n",
    "        )\n",
    "\n",
    "        print(f'Starting job {load_job.job_id}')\n",
    "        load_job.result()\n",
    "        print(f'Job finished.')\n",
    "\n",
    "        destination_table = client.get_table(table_id)\n",
    "        print(f'Loaded {destination_table.num_rows} rows.')\n",
    "```\n",
    "##### Análisis\n",
    "Se define función que permite ejecutar la consulta en bigquery.\n",
    "\n",
    "```python\n",
    "    # Se define función para ejecutar una consulta en BigQuery\n",
    "    def run_bigquery_query(query: str):\n",
    "        client = get_bigquery_client()\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "        return results\n",
    "```\n",
    "Se define la query SQL que permite ejecutar la consulta en bigquery.\n",
    "La lista de emojis se obtiene de https://gist.github.com/bfeldman89/fb25ddb63bdaa6de6ab7ac946acde96f#file-emojis-csv .\n",
    "```python\n",
    "    query = \"\"\"\n",
    "   WITH EmojiExtractor AS (\n",
    "    -- Separa todos los caracteres de cada tweet\n",
    "    SELECT\n",
    "        content,\n",
    "        SPLIT(content, '') AS chars\n",
    "    FROM\n",
    "        `project-latam-challenge.twitter_data.farmers_protest_tweets_2021`\n",
    "    ), EmojiCount AS (\n",
    "    -- Calcula la frecuencia de cada caracter\n",
    "    SELECT char,\n",
    "        COUNT(*) AS frequency\n",
    "    FROM EmojiExtractor\n",
    "    LEFT JOIN UNNEST(chars) AS char\n",
    "    WHERE\n",
    "    --LISTA DE POSIBLES EMOJIS\n",
    "        char IN (\n",
    "\"😀\",\"😁\",\"😂\",\"🤣\",\"😃\",\"😄\",\"😅\",\"😆\",\"😉\",\"😊\",\"😋\",\"😎\",\"😍\",\"😘\",\"😗\",\"😙\",\"😚\",\"☺\",\"🙂\",\"🤗\",\"🤩\",\"🤔\",\"🤨\",\"😐\",\"😑\",\"😶\",\"🙄\",\"😏\",\"😣\",\"😥\",\"😮\",\"🤐\",\"😯\",\"😪\",\"😫\",\"😴\",\"😌\",\"😛\",\"😜\",\"😝\",\"🤤\",\"😒\",\"😓\",\"😔\",\"😕\",\"🙃\",\"🤑\",\"😲\",\"☹\",\"🙁\",\"😖\",\"😞\",\"😟\",\"😤\",\"😢\",\"😭\",\"😦\",\"😧\",\"😨\",\"😩\",\"🤯\",\"😬\",\"😰\",\"😱\",\"😳\",\"🤪\",\"😵\",\"😡\",\"😠\",\"🤬\",\"😷\",\"🤒\",\"🤕\",\"🤢\",\"🤮\",\"🤧\",\"😇\",\"🤠\",\"🤡\",\"🤥\",\"🤫\",\"🤭\",\"🧐\",\"🤓\",\"😈\",\"👿\",\"👹\",\"👺\",\"💀\",\"👻\",\"👽\",\"🤖\",\"💩\",\"😺\",\"😸\",\"😹\",\"😻\",\"😼\",\"😽\",\"🙀\",\"😿\",\"😾\",\"👶\",\"👦\",\"👧\",\"👨\",\"👩\",\"👴\",\"👵\",\"👨‍⚕️\",\"👩‍⚕️\",\"👨‍🎓\",\"👩‍🎓\",\"👨‍⚖️\",\"👩‍⚖️\",\"👨‍🌾\",\"👩‍🌾\",\"👨‍🍳\",\"👩‍🍳\",\"👨‍🔧\",\"👩‍🔧\",\"👨‍🏭\",\"👩‍🏭\",\"👨‍💼\",\"👩‍💼\",\"👨‍🔬\",\"👩‍🔬\",\"👨‍💻\",\"👩‍💻\",\"👨‍🎤\",\"👩‍🎤\",\"👨‍🎨\",\"👩‍🎨\",\"👨‍✈️\",\"👩‍✈️\",\"👨‍🚀\",\"👩‍🚀\",\"👨‍🚒\",\"👩‍🚒\",\"👮\",\"👮‍♂️\",\"👮‍♀️\",\"🕵\",\"🕵️‍♂️\",\"🕵️‍♀️\",\"💂\",\"💂‍♂️\",\"💂‍♀️\",\"👷\",\"👷‍♂️\",\"👷‍♀️\",\"🤴\",\"👸\",\"👳\",\"👳‍♂️\",\"👳‍♀️\",\"👲\",\"🧕\",\"🧔\",\"👱\",\"👱‍♂️\",\"👱‍♀️\",\"🤵\",\"👰\",\"🤰\",\"🤱\",\"👼\",\"🎅\",\"🤶\",\"🧙‍♀️\",\"🧙‍♂️\",\"🧚‍♀️\",\"🧚‍♂️\",\"🧛‍♀️\",\"🧛‍♂️\",\"🧜‍♀️\",\"🧜‍♂️\",\"🧝‍♀️\",\"🧝‍♂️\",\"🧞‍♀️\",\"🧞‍♂️\",\"🧟‍♀️\",\"🧟‍♂️\",\"🙍\",\"🙍‍♂️\",\"🙍‍♀️\",\"🙎\",\"🙎‍♂️\",\"🙎‍♀️\",\"🙅\",\"🙅‍♂️\",\"🙅‍♀️\",\"🙆\",\"🙆‍♂️\",\"🙆‍♀️\",\"💁\",\"💁‍♂️\",\"💁‍♀️\",\"🙋\",\"🙋‍♂️\",\"🙋‍♀️\",\"🙇\",\"🙇‍♂️\",\"🙇‍♀️\",\"🤦\",\"🤦‍♂️\",\"🤦‍♀️\",\"🤷\",\"🤷‍♂️\",\"🤷‍♀️\",\"💆\",\"💆‍♂️\",\"💆‍♀️\",\"💇\",\"💇‍♂️\",\"💇‍♀️\",\"🚶\",\"🚶‍♂️\",\"🚶‍♀️\",\"🏃\",\"🏃‍♂️\",\"🏃‍♀️\",\"💃\",\"🕺\",\"👯\",\"👯‍♂️\",\"👯‍♀️\",\"🧖‍♀️\",\"🧖‍♂️\",\"🕴\",\"🗣\",\"👤\",\"👥\",\"👫\",\"👬\",\"👭\",\"💏\",\"👨‍❤️‍💋‍👨\",\"👩‍❤️‍💋‍👩\",\"💑\",\"👨‍❤️‍👨\",\"👩‍❤️‍👩\",\"👪\",\"👨‍👩‍👦\",\"👨‍👩‍👧\",\"👨‍👩‍👧‍👦\",\"👨‍👩‍👦‍👦\",\"👨‍👩‍👧‍👧\",\"👨‍👨‍👦\",\"👨‍👨‍👧\",\"👨‍👨‍👧‍👦\",\"👨‍👨‍👦‍👦\",\"👨‍👨‍👧‍👧\",\"👩‍👩‍👦\",\"👩‍👩‍👧\",\"👩‍👩‍👧‍👦\",\"👩‍👩‍👦‍👦\",\"👩‍👩‍👧‍👧\",\"👨‍👦\",\"👨‍👦‍👦\",\"👨‍👧\",\"👨‍👧‍👦\",\"👨‍👧‍👧\",\"👩‍👦\",\"👩‍👦‍👦\",\"👩‍👧\",\"👩‍👧‍👦\",\"👩‍👧‍👧\",\"🤳\",\"💪\",\"👈\",\"👉\",\"☝\",\"👆\",\"🖕\",\"👇\",\"✌\",\"🤞\",\"🖖\",\"🤘\",\"🖐\",\"✋\",\"👌\",\"👍\",\"👎\",\"✊\",\"👊\",\"🤛\",\"🤜\",\"🤚\",\"👋\",\"🤟\",\"✍\",\"👏\",\"👐\",\"🙌\",\"🤲\",\"🙏\",\"🤝\",\"💅\",\"👂\",\"👃\",\"👣\",\"👀\",\"👁\",\"🧠\",\"👅\",\"👄\",\"💋\",\"👓\",\"🕶\",\"👔\",\"👕\",\"👖\",\"🧣\",\"🧤\",\"🧥\",\"🧦\",\"👗\",\"👘\",\"👙\",\"👚\",\"👛\",\"👜\",\"👝\",\"🎒\",\"👞\",\"👟\",\"👠\",\"👡\",\"👢\",\"👑\",\"👒\",\"🎩\",\"🎓\",\"🧢\",\"⛑\",\"💄\",\"💍\",\"🌂\",\"☂\",\"💼\",\"break\",\"🙈\",\"🙉\",\"🙊\",\"💥\",\"💦\",\"💨\",\"💫\",\"🐵\",\"🐒\",\"🦍\",\"🐶\",\"🐕\",\"🐩\",\"🐺\",\"🦊\",\"🐱\",\"🐈\",\"🦁\",\"🐯\",\"🐅\",\"🐆\",\"🐴\",\"🐎\",\"🦄\",\"🦓\",\"🐮\",\"🐂\",\"🐃\",\"🐄\",\"🐷\",\"🐖\",\"🐗\",\"🐽\",\"🐏\",\"🐑\",\"🐐\",\"🐪\",\"🐫\",\"🦒\",\"🐘\",\"🦏\",\"🐭\",\"🐁\",\"🐀\",\"🐹\",\"🐰\",\"🐇\",\"🐿\",\"🦔\",\"🦇\",\"🐻\",\"🐨\",\"🐼\",\"🐾\",\"🦃\",\"🐔\",\"🐓\",\"🐣\",\"🐤\",\"🐥\",\"🐦\",\"🐧\",\"🕊\",\"🦅\",\"🦆\",\"🦉\",\"🐸\",\"🐊\",\"🐢\",\"🦎\",\"🐍\",\"🐲\",\"🐉\",\"🦕\",\"🦖\",\"🐳\",\"🐋\",\"🐬\",\"🐟\",\"🐠\",\"🐡\",\"🦈\",\"🐙\",\"🐚\",\"🦀\",\"🦐\",\"🦑\",\"🐌\",\"🦋\",\"🐛\",\"🐜\",\"🐝\",\"🐞\",\"🦗\",\"🕷\",\"🕸\",\"🦂\",\"💐\",\"🌸\",\"💮\",\"🏵\",\"🌹\",\"🥀\",\"🌺\",\"🌻\",\"🌼\",\"🌷\",\"🌱\",\"🌲\",\"🌳\",\"🌴\",\"🌵\",\"🌾\",\"🌿\",\"☘\",\"🍀\",\"🍁\",\"🍂\",\"🍃\",\"🍄\",\"🌰\",\"🌍\",\"🌎\",\"🌏\",\"🌐\",\"🌑\",\"🌒\",\"🌓\",\"🌔\",\"🌕\",\"🌖\",\"🌗\",\"🌘\",\"🌙\",\"🌚\",\"🌛\",\"🌜\",\"☀\",\"🌝\",\"🌞\",\"⭐\",\"🌟\",\"🌠\",\"☁\",\"⛅\",\"⛈\",\"🌤\",\"🌥\",\"🌦\",\"🌧\",\"🌨\",\"🌩\",\"🌪\",\"🌫\",\"🌬\",\"🌈\",\"☂\",\"☔\",\"⚡\",\"❄\",\"☃\",\"⛄\",\"☄\",\"🔥\",\"💧\",\"🌊\",\"🎄\",\"✨\",\"🎋\",\"🎍\",\"break\",\"🍇\",\"🍈\",\"🍉\",\"🍊\",\"🍋\",\"🍌\",\"🍍\",\"🍎\",\"🍏\",\"🍐\",\"🍑\",\"🍒\",\"🍓\",\"🥝\",\"🍅\",\"🥥\",\"🥑\",\"🍆\",\"🥔\",\"🥕\",\"🌽\",\"🌶\",\"🥒\",\"🥦\",\"🍄\",\"🥜\",\"🌰\",\"🍞\",\"🥐\",\"🥖\",\"🥨\",\"🥞\",\"🧀\",\"🍖\",\"🍗\",\"🥩\",\"🥓\",\"🍔\",\"🍟\",\"🍕\",\"🌭\",\"🥪\",\"🌮\",\"🌯\",\"🍳\",\"🍲\",\"🥣\",\"🥗\",\"🍿\",\"🥫\",\"🍱\",\"🍘\",\"🍙\",\"🍚\",\"🍛\",\"🍜\",\"🍝\",\"🍠\",\"🍢\",\"🍣\",\"🍤\",\"🍥\",\"🍡\",\"🥟\",\"🥠\",\"🥡\",\"🍦\",\"🍧\",\"🍨\",\"🍩\",\"🍪\",\"🎂\",\"🍰\",\"🥧\",\"🍫\",\"🍬\",\"🍭\",\"🍮\",\"🍯\",\"🍼\",\"🥛\",\"☕\",\"🍵\",\"🍶\",\"🍾\",\"🍷\",\"🍸\",\"🍹\",\"🍺\",\"🍻\",\"🥂\",\"🥃\",\"🥤\",\"🥢\",\"🍽\",\"🍴\",\"🥄\",\"break\",\"👾\",\"🧗‍♀️\",\"🧗‍♂️\",\"🧘‍♀️\",\"🧘‍♂️\",\"🕴\",\"🏇\",\"⛷\",\"🏂\",\"🏌\",\"🏌️‍♂️\",\"🏌️‍♀️\",\"🏄\",\"🏄‍♂️\",\"🏄‍♀️\",\"🚣\",\"🚣‍♂️\",\"🚣‍♀️\",\"🏊\",\"🏊‍♂️\",\"🏊‍♀️\",\"⛹\",\"⛹️‍♂️\",\"⛹️‍♀️\",\"🏋\",\"🏋️‍♂️\",\"🏋️‍♀️\",\"🚴\",\"🚴‍♂️\",\"🚴‍♀️\",\"🚵\",\"🚵‍♂️\",\"🚵‍♀️\",\"🤸\",\"🤸‍♂️\",\"🤸‍♀️\",\"🤼\",\"🤼‍♂️\",\"🤼‍♀️\",\"🤽\",\"🤽‍♂️\",\"🤽‍♀️\",\"🤾\",\"🤾‍♂️\",\"🤾‍♀️\",\"🤹\",\"🤹‍♂️\",\"🤹‍♀️\",\"🎪\",\"🎗\",\"🎟\",\"🎫\",\"🎖\",\"🏆\",\"🏅\",\"🥇\",\"🥈\",\"🥉\",\"⚽\",\"⚾\",\"🏀\",\"🏐\",\"🏈\",\"🏉\",\"🎾\",\"🎳\",\"🏏\",\"🏑\",\"🏒\",\"🏓\",\"🏸\",\"🥊\",\"🥋\",\"⛳\",\"⛸\",\"🎣\",\"🎽\",\"🎿\",\"🛷\",\"🥌\",\"🎯\",\"🎱\",\"🎮\",\"🎰\",\"🎲\",\"🎭\",\"🎨\",\"🎼\",\"🎤\",\"🎧\",\"🎷\",\"🎸\",\"🎹\",\"🎺\",\"🎻\",\"🥁\",\"🎬\",\"🏹\",\"break\",\"🚣\",\"🏎\",\"🏍\",\"🗾\",\"🏔\",\"⛰\",\"🌋\",\"🗻\",\"🏕\",\"🏖\",\"🏜\",\"🏝\",\"🏞\",\"🏟\",\"🏛\",\"🏗\",\"🏘\",\"🏚\",\"🏠\",\"🏡\",\"🏢\",\"🏣\",\"🏤\",\"🏥\",\"🏦\",\"🏨\",\"🏩\",\"🏪\",\"🏫\",\"🏬\",\"🏭\",\"🏯\",\"🏰\",\"💒\",\"🗼\",\"🗽\",\"⛪\",\"🕌\",\"🕍\",\"⛩\",\"🕋\",\"⛲\",\"⛺\",\"🌁\",\"🌃\",\"🏙\",\"🌄\",\"🌅\",\"🌆\",\"🌇\",\"🌉\",\"🌌\",\"🎠\",\"🎡\",\"🎢\",\"🚂\",\"🚃\",\"🚄\",\"🚅\",\"🚆\",\"🚇\",\"🚈\",\"🚉\",\"🚊\",\"🚝\",\"🚞\",\"🚋\",\"🚌\",\"🚍\",\"🚎\",\"🚐\",\"🚑\",\"🚒\",\"🚓\",\"🚔\",\"🚕\",\"🚖\",\"🚗\",\"🚘\",\"🚚\",\"🚛\",\"🚜\",\"🚲\",\"🛴\",\"🛵\",\"🚏\",\"🛤\",\"⛽\",\"🚨\",\"🚥\",\"🚦\",\"🚧\",\"⚓\",\"⛵\",\"🚤\",\"🛳\",\"⛴\",\"🛥\",\"🚢\",\"✈\",\"🛩\",\"🛫\",\"🛬\",\"💺\",\"🚁\",\"🚟\",\"🚠\",\"🚡\",\"🛰\",\"🚀\",\"🛸\",\"🌠\",\"⛱\",\"🎆\",\"🎇\",\"🎑\",\"💴\",\"💵\",\"💶\",\"💷\",\"🗿\",\"🛂\",\"🛃\",\"🛄\",\"🛅\",\"break\",\"☠\",\"🛀\",\"🛌\",\"💌\",\"💣\",\"🕳\",\"🛍\",\"📿\",\"💎\",\"🔪\",\"🏺\",\"🗺\",\"💈\",\"🛢\",\"🛎\",\"⌛\",\"⏳\",\"⌚\",\"⏰\",\"⏱\",\"⏲\",\"🕰\",\"🌡\",\"⛱\",\"🎈\",\"🎉\",\"🎊\",\"🎎\",\"🎏\",\"🎐\",\"🎀\",\"🎁\",\"🔮\",\"🕹\",\"🖼\",\"📯\",\"🎙\",\"🎚\",\"🎛\",\"📻\",\"📱\",\"📲\",\"☎\",\"📞\",\"📟\",\"📠\",\"🔋\",\"🔌\",\"💻\",\"🖥\",\"🖨\",\"⌨\",\"🖱\",\"🖲\",\"💽\",\"💾\",\"💿\",\"📀\",\"🎥\",\"🎞\",\"📽\",\"📺\",\"📷\",\"📸\",\"📹\",\"📼\",\"🔍\",\"🔎\",\"🕯\",\"💡\",\"🔦\",\"🏮\",\"📔\",\"📕\",\"📖\",\"📗\",\"📘\",\"📙\",\"📚\",\"📓\",\"📃\",\"📜\",\"📄\",\"📰\",\"🗞\",\"📑\",\"🔖\",\"🏷\",\"💰\",\"💴\",\"💵\",\"💶\",\"💷\",\"💸\",\"💳\",\"✉\",\"📧\",\"📨\",\"📩\",\"📤\",\"📥\",\"📦\",\"📫\",\"📪\",\"📬\",\"📭\",\"📮\",\"🗳\",\"✏\",\"✒\",\"🖋\",\"🖊\",\"🖌\",\"🖍\",\"📝\",\"📁\",\"📂\",\"🗂\",\"📅\",\"📆\",\"🗒\",\"🗓\",\"📇\",\"📈\",\"📉\",\"📊\",\"📋\",\"📌\",\"📍\",\"📎\",\"🖇\",\"📏\",\"📐\",\"✂\",\"🗃\",\"🗄\",\"🗑\",\"🔒\",\"🔓\",\"🔏\",\"🔐\",\"🔑\",\"🗝\",\"🔨\",\"⛏\",\"⚒\",\"🛠\",\"🗡\",\"⚔\",\"🔫\",\"🛡\",\"🔧\",\"🔩\",\"⚙\",\"🗜\",\"⚖\",\"🔗\",\"⛓\",\"⚗\",\"🔬\",\"🔭\",\"📡\",\"💉\",\"💊\",\"🚪\",\"🛏\",\"🛋\",\"🚽\",\"🚿\",\"🛁\",\"🚬\",\"⚰\",\"⚱\",\"🗿\",\"🚰\",\"break\",\"👁️‍🗨️\",\"💘\",\"❤\",\"💓\",\"💔\",\"💕\",\"💖\",\"💗\",\"💙\",\"💚\",\"💛\",\"🧡\",\"💜\",\"🖤\",\"💝\",\"💞\",\"💟\",\"❣\",\"💤\",\"💢\",\"💬\",\"🗯\",\"💭\",\"💮\",\"♨\",\"💈\",\"🛑\",\"🕛\",\"🕧\",\"🕐\",\"🕜\",\"🕑\",\"🕝\",\"🕒\",\"🕞\",\"🕓\",\"🕟\",\"🕔\",\"🕠\",\"🕕\",\"🕡\",\"🕖\",\"🕢\",\"🕗\",\"🕣\",\"🕘\",\"🕤\",\"🕙\",\"🕥\",\"🕚\",\"🕦\",\"🌀\",\"♠\",\"♥\",\"♦\",\"♣\",\"🃏\",\"🀄\",\"🎴\",\"🔇\",\"🔈\",\"🔉\",\"🔊\",\"📢\",\"📣\",\"📯\",\"🔔\",\"🔕\",\"🎵\",\"🎶\",\"🏧\",\"🚮\",\"🚰\",\"♿\",\"🚹\",\"🚺\",\"🚻\",\"🚼\",\"🚾\",\"⚠\",\"🚸\",\"⛔\",\"🚫\",\"🚳\",\"🚭\",\"🚯\",\"🚱\",\"🚷\",\"🔞\",\"☢\",\"☣\",\"⬆\",\"↗\",\"➡\",\"↘\",\"⬇\",\"↙\",\"⬅\",\"↖\",\"↕\",\"↔\",\"↩\",\"↪\",\"⤴\",\"⤵\",\"🔃\",\"🔄\",\"🔙\",\"🔚\",\"🔛\",\"🔜\",\"🔝\",\"🛐\",\"⚛\",\"🕉\",\"✡\",\"☸\",\"☯\",\"✝\",\"☦\",\"☪\",\"☮\",\"🕎\",\"🔯\",\"♈\",\"♉\",\"♊\",\"♋\",\"♌\",\"♍\",\"♎\",\"♏\",\"♐\",\"♑\",\"♒\",\"♓\",\"⛎\",\"🔀\",\"🔁\",\"🔂\",\"▶\",\"⏩\",\"◀\",\"⏪\",\"🔼\",\"⏫\",\"🔽\",\"⏬\",\"⏹\",\"⏏\",\"🎦\",\"🔅\",\"🔆\",\"📶\",\"📳\",\"📴\",\"♻\",\"🔱\",\"📛\",\"🔰\",\"⭕\",\"✅\",\"☑\",\"✔\",\"✖\",\"❌\",\"❎\",\"➕\",\"➖\",\"➗\",\"➰\",\"➿\",\"〽\",\"✳\",\"✴\",\"❇\",\"‼\",\"⁉\",\"❓\",\"❔\",\"❕\",\"❗\",\"©\",\"®\",\"™\",\"️⃣\",\"0️⃣\",\"1️⃣\",\"2️⃣\",\"3️⃣\",\"4️⃣\",\"5️⃣\",\"6️⃣\",\"7️⃣\",\"8️⃣\",\"9️⃣\",\"🔟\",\"💯\",\"🔠\",\"🔡\",\"🔢\",\"🔣\",\"🔤\",\"🅰\",\"🆎\",\"🅱\",\"🆑\",\"🆒\",\"🆓\",\"ℹ\",\"🆔\",\"Ⓜ\",\"🆕\",\"🆖\",\"🅾\",\"🆗\",\"🅿\",\"🆘\",\"🆙\",\"🆚\",\"🈁\",\"🈂\",\"🈷\",\"🈶\",\"🈯\",\"🉐\",\"🈹\",\"🈚\",\"🈲\",\"🉑\",\"🈸\",\"🈴\",\"🈳\",\"㊗\",\"㊙\",\"🈺\",\"🈵\",\"▪\",\"▫\",\"◻\",\"◼\",\"◽\",\"◾\",\"⬛\",\"⬜\",\"🔶\",\"🔷\",\"🔸\",\"🔹\",\"🔺\",\"🔻\",\"💠\",\"🔲\",\"🔳\",\"⚪\",\"⚫\",\"🔴\",\"🔵\",\"break\",\"🏁\",\"🚩\",\"🎌\",\"🏴\",\"🏳\",\"🏳️‍🌈\",\"🏴‍☠️\",\"🇦🇨\",\"🇦🇩\",\"🇦🇪\",\"🇦🇫\",\"🇦🇬\",\"🇦🇮\",\"🇦🇱\",\"🇦🇲\",\"🇦🇴\",\"🇦🇶\",\"🇦🇷\",\"🇦🇸\",\"🇦🇹\",\"🇦🇺\",\"🇦🇼\",\"🇦🇽\",\"🇦🇿\",\"🇧🇦\",\"🇧🇧\",\"🇧🇩\",\"🇧🇪\",\"🇧🇫\",\"🇧🇬\",\"🇧🇭\",\"🇧🇮\",\"🇧🇯\",\"🇧🇱\",\"🇧🇲\",\"🇧🇳\",\"🇧🇴\",\"🇧🇶\",\"🇧🇷\",\"🇧🇸\",\"🇧🇹\",\"🇧🇻\",\"🇧🇼\",\"🇧🇾\",\"🇧🇿\",\"🇨🇦\",\"🇨🇨\",\"🇨🇩\",\"🇨🇫\",\"🇨🇬\",\"🇨🇭\",\"🇨🇮\",\"🇨🇰\",\"🇨🇱\",\"🇨🇲\",\"🇨🇳\",\"🇨🇴\",\"🇨🇵\",\"🇨🇷\",\"🇨🇺\",\"🇨🇻\",\"🇨🇼\",\"🇨🇽\",\"🇨🇾\",\"🇨🇿\",\"🇩🇪\",\"🇩🇬\",\"🇩🇯\",\"🇩🇰\",\"🇩🇲\",\"🇩🇴\",\"🇩🇿\",\"🇪🇦\",\"🇪🇨\",\"🇪🇪\",\"🇪🇬\",\"🇪🇭\",\"🇪🇷\",\"🇪🇸\",\"🇪🇹\",\"🇪🇺\",\"🇫🇮\",\"🇫🇯\",\"🇫🇰\",\"🇫🇲\",\"🇫🇴\",\"🇫🇷\",\"🇬🇦\",\"🇬🇧\",\"🇬🇩\",\"🇬🇪\",\"🇬🇫\",\"🇬🇬\",\"🇬🇭\",\"🇬🇮\",\"🇬🇱\",\"🇬🇲\",\"🇬🇳\",\"🇬🇵\",\"🇬🇶\",\"🇬🇷\",\"🇬🇸\",\"🇬🇹\",\"🇬🇺\",\"🇬🇼\",\"🇬🇾\",\"🇭🇰\",\"🇭🇲\",\"🇭🇳\",\"🇭🇷\",\"🇭🇹\",\"🇭🇺\",\"🇮🇨\",\"🇮🇩\",\"🇮🇪\",\"🇮🇱\",\"🇮🇲\",\"🇮🇳\",\"🇮🇴\",\"🇮🇶\",\"🇮🇷\",\"🇮🇸\",\"🇮🇹\",\"🇯🇪\",\"🇯🇲\",\"🇯🇴\",\"🇯🇵\",\"🇰🇪\",\"🇰🇬\",\"🇰🇭\",\"🇰🇮\",\"🇰🇲\",\"🇰🇳\",\"🇰🇵\",\"🇰🇷\",\"🇰🇼\",\"🇰🇾\",\"🇰🇿\",\"🇱🇦\",\"🇱🇧\",\"🇱🇨\",\"🇱🇮\",\"🇱🇰\",\"🇱🇷\",\"🇱🇸\",\"🇱🇹\",\"🇱🇺\",\"🇱🇻\",\"🇱🇾\",\"🇲🇦\",\"🇲🇨\",\"🇲🇩\",\"🇲🇪\",\"🇲🇫\",\"🇲🇬\",\"🇲🇭\",\"🇲🇰\",\"🇲🇱\",\"🇲🇲\",\"🇲🇳\",\"🇲🇴\",\"🇲🇵\",\"🇲🇶\",\"🇲🇷\",\"🇲🇸\",\"🇲🇹\",\"🇲🇺\",\"🇲🇻\",\"🇲🇼\",\"🇲🇽\",\"🇲🇾\",\"🇲🇿\",\"🇳🇦\",\"🇳🇨\",\"🇳🇪\",\"🇳🇫\",\"🇳🇬\",\"🇳🇮\",\"🇳🇱\",\"🇳🇴\",\"🇳🇵\",\"🇳🇷\",\"🇳🇺\",\"🇳🇿\",\"🇴🇲\",\"🇵🇦\",\"🇵🇪\",\"🇵🇫\",\"🇵🇬\",\"🇵🇭\",\"🇵🇰\",\"🇵🇱\",\"🇵🇲\",\"🇵🇳\",\"🇵🇷\",\"🇵🇸\",\"🇵🇹\",\"🇵🇼\",\"🇵🇾\",\"🇶🇦\",\"🇷🇪\",\"🇷🇴\",\"🇷🇸\",\"🇷🇺\",\"🇷🇼\",\"🇸🇦\",\"🇸🇧\",\"🇸🇨\",\"🇸🇩\",\"🇸🇪\",\"🇸🇬\",\"🇸🇭\",\"🇸🇮\",\"🇸🇯\",\"🇸🇰\",\"🇸🇱\",\"🇸🇲\",\"🇸🇳\",\"🇸🇴\",\"🇸🇷\",\"🇸🇸\",\"🇸🇹\",\"🇸🇻\",\"🇸🇽\",\"🇸🇾\",\"🇸🇿\",\"🇹🇦\",\"🇹🇨\",\"🇹🇩\",\"🇹🇫\",\"🇹🇬\",\"🇹🇭\",\"🇹🇯\",\"🇹🇰\",\"🇹🇱\",\"🇹🇲\",\"🇹🇳\",\"🇹🇴\",\"🇹🇷\",\"🇹🇹\",\"🇹🇻\",\"🇹🇼\",\"🇹🇿\",\"🇺🇦\",\"🇺🇬\",\"🇺🇲\",\"🇺🇳\",\"🇺🇸\",\"🇺🇾\",\"🇺🇿\",\"🇻🇦\",\"🇻🇨\",\"🇻🇪\",\"🇻🇬\",\"🇻🇮\",\"🇻🇳\",\"🇻🇺\",\"🇼🇫\",\"🇼🇸\",\"🇽🇰\",\"🇾🇪\",\"🇾🇹\",\"🇿🇦\",\"🇿🇲\",\"🇿🇼\",\"🏴󠁧󠁢󠁥󠁮󠁧󠁿\",\"🏴󠁧󠁢󠁳󠁣󠁴󠁿\",\"🏴󠁧󠁢󠁷󠁬󠁳󠁿\"\n",
    "        )\n",
    "    GROUP BY char\n",
    "    )\n",
    "    SELECT char AS emoji,\n",
    "    frequency\n",
    "    FROM EmojiCount\n",
    "    ORDER BY frequency DESC\n",
    "    LIMIT 10\n",
    "        \"\"\"\n",
    "    \n",
    "    results = run_bigquery_query(query)\n",
    "    return [(row.fecha, row.username) for row in results]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso q1_time...\n",
      "Leyendo archivo JSON\n",
      "Creando bucket y subiendo archivo\n",
      "Bucket bucket-project-latam-challenge-q2-time created in location US.\n",
      "File farmers-protest-tweets-2021-2-4.json uploaded to bucket-project-latam-challenge-q2-time/farmers-protest-tweets-2021-2-4.json.\n",
      "Cargando datos a BigQuery\n",
      "Starting job 1333d214-6ca2-451e-b71b-4424be85fa81\n",
      "Job finished.\n",
      "Loaded 234814 rows.\n",
      "Ejecutando query\n",
      "Resultados:\n",
      "[('✊', 4822), ('❤', 3558), ('☮', 632), ('✌', 548), ('‼', 222), ('✍', 206), ('♥', 162), ('⛳', 136), ('✅', 114), ('➡', 112)]\n",
      "Tiempo de ejecución: 51.47749996185303 segundos\n",
      "Uso máximo de memoria: 2133.34765625 MiB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from q2_time import q2_time\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"../credentials/project-latam-challenge-749ce1a96052.json\"\n",
    "\n",
    "# Función para medir el tiempo y la memoria de q2_time\n",
    "def measure_q1_time(file_path):\n",
    "    start_time = time.time()\n",
    "    mem_usage, result = memory_usage((q2_time, (file_path,)), retval=True, max_usage=True)\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    return result, duration, mem_usage\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Iniciando proceso q1_time...\")\n",
    "    top_10_emojis, duration, mem_usage = measure_q1_time(file_path)\n",
    "    print(\"Resultados:\")\n",
    "    print(top_10_emojis)\n",
    "    print(f\"Tiempo de ejecución: {duration} segundos\")\n",
    "    print(f\"Uso máximo de memoria: {mem_usage} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREGUNTA 2 - MEMORY\n",
    "Se propone generar un proceso análogo al anteriormente realizado para la versión time, pero con una carga parcelada de forma que utilice menos memoria local y se ejecute con menor necesidad de recursos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos. Debe incluir las siguientes funciones:\n",
    "```python\n",
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "```\n",
    "```python\n",
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "```\n",
    "```python\n",
    "Returns: \n",
    "[(\"LATAM321\", 387), (\"LATAM_CHI\", 129), ...]\n",
    "```\n",
    "​\n",
    "## PREGUNTA 3 - TIME (q3_time)\n",
    "Para desarrollar la solución a este problema se utilizará una solución cloud basada en Google Cloud Platform. La función contendrá el proceso de ETL para llevar los datos desde el archivo json local hacia Google Cloud Storage y posteriormente modelar esa data en una tabla de bigquery que permita realizar consultas de manera rápida y eficiente.\n",
    "### ETL\n",
    "El proceso de extracción, transformación y carga de los archivos en GCP permite llevar los archivos a una plataforma de performance rápida y efectiva.\n",
    "#### Credenciales\n",
    "Este desarrollo se realizará utilizando Google Cloud, por lo que se crea un proyecto en GCP llamado \"project-latam-challenge\". En este proyecto se crea una service account llamada \"sa-etl-latam-challenge\" que será utilizada para realizar la carga de datos.\n",
    "```python\n",
    "import os\n",
    "\n",
    "# Ruta a archivo de credenciales JSON de Google Cloud\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"../credentials/project-latam-challenge-749ce1a96052.json\"\n",
    "```\n",
    "#### Carga en Dataframe\n",
    "Se carga en un dataframe el [archivo](https://drive.google.com/file/d/1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis/view?usp=sharing) json declarado como parte del challenge.\n",
    "```python\n",
    "    # Leer el archivo CSV en un DataFrame\n",
    "    df = pd.read_json(file_path,lines=True)\n",
    "```\n",
    "#### Carga de dataframe en GCP\n",
    "##### Carga de archivo en Google Cloud Storage (GCS)\n",
    "Usando Google Cloud SDK se crea un bucket llamado 'bucket-project-latam-challenge-q1-time' en el proyecto 'project-latam-challenge' y se carga el archivo directamente a dicho bucket.\n",
    "```python\n",
    "    # Se define función que permite cargar un archivo en un bucket de Google Cloud Storage\n",
    "    def create_bucket_and_upload_file(project_id, bucket_name, file_path, destination_blob_name):\n",
    "        # Inicializar el cliente de almacenamiento\n",
    "        storage_client = storage.Client(project=project_id)\n",
    "        \n",
    "        # Verificar si el bucket ya existe\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        if not bucket.exists():\n",
    "            # Crear un nuevo bucket con la ubicación especificada\n",
    "            new_bucket = storage_client.create_bucket(bucket, location=\"US\")\n",
    "            print(f'Bucket {bucket_name} created in location US.')\n",
    "        else:\n",
    "            print(f'Bucket {bucket_name} already exists.')\n",
    "            new_bucket = bucket\n",
    "        \n",
    "        # Subir el archivo al bucket\n",
    "        blob = new_bucket.blob(destination_blob_name)\n",
    "        blob.upload_from_filename(file_path)\n",
    "        print(f'File {file_path} uploaded to {bucket_name}/{destination_blob_name}.')\n",
    "```\n",
    "##### Configuración de carga\n",
    "```python\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Parámetros\n",
    "project_id = 'project-latam-challenge'\n",
    "dataset_id = 'twitter_data'\n",
    "table_id = 'farmers_protest_tweets_2021'\n",
    "```\n",
    "##### Declaración de schema\n",
    "Se declara explicitamente la estructura del esquema con la data correspondiente, esto permitirá evitar errores en los tipos de dato al cargar la data en Bigquery.\n",
    "Por motivos de claridad al momento de leer el markdown, se omite el schema que se puede encontrar en el archivo q1_time.py .\n",
    "##### Proceso de creación de tabla en bigquery\n",
    "```python\n",
    "    # Se define función que crea tabla de bigquery a partir de archivo almacenado en GCS\n",
    "    def load_data_from_gcs_to_bigquery(uri, table_id):\n",
    "        # Inicializa el cliente de BigQuery\n",
    "        client = bigquery.Client()\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            schema=schema,\n",
    "            source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "            max_bad_records=0,  # No permitir registros malos antes de fallar\n",
    "            time_partitioning=bigquery.TimePartitioning(\n",
    "                type_=bigquery.TimePartitioningType.DAY,\n",
    "                field=\"date\"  # Campo de partición\n",
    "            )\n",
    "        )\n",
    "\n",
    "        load_job = client.load_table_from_uri(\n",
    "            uri, table_id, job_config=job_config\n",
    "        )\n",
    "\n",
    "        print(f'Starting job {load_job.job_id}')\n",
    "        load_job.result()\n",
    "        print(f'Job finished.')\n",
    "\n",
    "        destination_table = client.get_table(table_id)\n",
    "        print(f'Loaded {destination_table.num_rows} rows.')\n",
    "```\n",
    "##### Análisis\n",
    "Se define función que permite ejecutar la consulta en bigquery.\n",
    "\n",
    "```python\n",
    "    # Se define función para ejecutar una consulta en BigQuery\n",
    "    def run_bigquery_query(query: str):\n",
    "        client = get_bigquery_client()\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "        return results\n",
    "```\n",
    "Se define la query SQL que permite ejecutar la consulta en bigquery.\n",
    "La lista de emojis se obtiene de https://gist.github.com/bfeldman89/fb25ddb63bdaa6de6ab7ac946acde96f#file-emojis-csv .\n",
    "```python\n",
    "    query = \"\"\"\n",
    "        WITH TEMP_DATA_001 AS \n",
    "        (\n",
    "        SELECT mentionedUsers.username AS username\n",
    "            -- quotedTweet.mentionedUsers.username,\n",
    "            -- quotedTweet.quotedTweet.mentionedUsers.username,\n",
    "            -- quotedTweet.quotedTweet.quotedTweet.mentionedUsers.username\n",
    "        FROM `project-latam-challenge.twitter_data.farmers_protest_tweets_2021` \n",
    "        LEFT JOIN UNNEST(mentionedUsers) AS mentionedUsers\n",
    "        WHERE date is not null AND mentionedUsers.username IS NOT NULL\n",
    "        UNION ALL\n",
    "\n",
    "        SELECT quotedTweet_mentionedUsers.username AS username\n",
    "            -- quotedTweet.quotedTweet.mentionedUsers.username,\n",
    "            -- quotedTweet.quotedTweet.quotedTweet.mentionedUsers.username\n",
    "        FROM `project-latam-challenge.twitter_data.farmers_protest_tweets_2021` \n",
    "        LEFT JOIN UNNEST(quotedTweet.mentionedUsers) AS quotedTweet_mentionedUsers\n",
    "        WHERE date is not null AND quotedTweet_mentionedUsers.username IS NOT NULL\n",
    "\n",
    "        UNION ALL\n",
    "\n",
    "        SELECT quotedTweet_quotedTweet_mentionedUsers.username AS username\n",
    "            -- quotedTweet.quotedTweet.quotedTweet.mentionedUsers.username\n",
    "        FROM `project-latam-challenge.twitter_data.farmers_protest_tweets_2021` \n",
    "        LEFT JOIN UNNEST(quotedTweet.quotedTweet.mentionedUsers) AS quotedTweet_quotedTweet_mentionedUsers\n",
    "        WHERE date is not null AND quotedTweet_quotedTweet_mentionedUsers.username IS NOT NULL\n",
    "\n",
    "        UNION ALL\n",
    "\n",
    "        SELECT quotedTweet_quotedTweet_quotedTweet_mentionedUsers.username AS username\n",
    "            -- quotedTweet.quotedTweet.quotedTweet.mentionedUsers.username\n",
    "        FROM `project-latam-challenge.twitter_data.farmers_protest_tweets_2021` \n",
    "        LEFT JOIN UNNEST(quotedTweet.quotedTweet.quotedTweet.mentionedUsers) AS quotedTweet_quotedTweet_quotedTweet_mentionedUsers\n",
    "        WHERE date is not null AND quotedTweet_quotedTweet_quotedTweet_mentionedUsers.username IS NOT NULL\n",
    "        )\n",
    "        SELECT username,\n",
    "        COUNT(*) AS frequency\n",
    "        FROM TEMP_DATA_001 A\n",
    "        GROUP BY ALL\n",
    "        ORDER BY frequency DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "    \n",
    "    results = run_bigquery_query(query)\n",
    "    return [(row.fecha, row.username) for row in results]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso q1_time...\n",
      "Leyendo archivo JSON\n",
      "Creando bucket y subiendo archivo\n",
      "Bucket bucket-project-latam-challenge-q3-time created in location US.\n",
      "File farmers-protest-tweets-2021-2-4.json uploaded to bucket-project-latam-challenge-q3-time/farmers-protest-tweets-2021-2-4.json.\n",
      "Cargando datos a BigQuery\n",
      "Starting job 1796d4db-1b8c-4d54-a87d-27d18e14ae1b\n",
      "Job finished.\n",
      "Loaded 352221 rows.\n",
      "Ejecutando query\n",
      "Resultados:\n",
      "[('narendramodi', 7875), ('Kisanektamorcha', 6135), ('RakeshTikaitBKU', 5544), ('PMOIndia', 4686), ('GretaThunberg', 3939), ('RahulGandhi', 3756), ('rihanna', 3471), ('DelhiPolice', 3420), ('RaviSinghKA', 3381), ('UNHumanRights', 3180)]\n",
      "Tiempo de ejecución: 50.49053502082825 segundos\n",
      "Uso máximo de memoria: 2575.734375 MiB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from q3_time import q3_time\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"../credentials/project-latam-challenge-749ce1a96052.json\"\n",
    "\n",
    "# Función para medir el tiempo y la memoria de q2_time\n",
    "def measure_q1_time(file_path):\n",
    "    start_time = time.time()\n",
    "    mem_usage, result = memory_usage((q3_time, (file_path,)), retval=True, max_usage=True)\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    return result, duration, mem_usage\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Iniciando proceso q1_time...\")\n",
    "    top_10_users, duration, mem_usage = measure_q1_time(file_path)\n",
    "    print(\"Resultados:\")\n",
    "    print(top_10_users)\n",
    "    print(f\"Tiempo de ejecución: {duration} segundos\")\n",
    "    print(f\"Uso máximo de memoria: {mem_usage} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### CONCLUSIÓN GENERAL\n",
    "GCP es una herramienta adecuada para la realización de este tipo de tareas, debido a que tiene bueno tiempos de respuesta, pese a un bajo uso de memoria local, esto se logra principalmente gracias a la infraestructura de procesamiento distribuido que utiliza Bigquery.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
}
